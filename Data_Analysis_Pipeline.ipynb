{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPypaKh9A2aZ0UeQiEfotAK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muajnstu/Bank-Marketing-using-rough-set-approach/blob/main/Data_Analysis_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install k_means_constrained"
      ],
      "metadata": {
        "id": "Lj-KZr-NxtjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from k_means_constrained import KMeansConstrained\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "Odes6oLMwOxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "universal_bank_dataset = \"https://media.githubusercontent.com/media/shahriariit/opendataset/refs/heads/master/UniversalBank.csv\"\n",
        "df = pd.read_csv(universal_bank_dataset)"
      ],
      "metadata": {
        "id": "ojtYxzOEwRwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['Personal Loan'])\n",
        "y = df['Personal Loan']\n",
        "\n",
        "print(f\"Dataset shape   : {df.shape}\")\n",
        "print(f\"Class 0 samples : {(y==0).sum()}\")\n",
        "print(f\"Class 1 samples : {(y==1).sum()}\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7UnZpmnwwHjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_0 = X[y == 0].copy()\n",
        "X_1 = X[y == 1].copy()\n",
        "\n",
        "scaler_0 = StandardScaler()\n",
        "scaler_1 = StandardScaler()\n",
        "\n",
        "X_0_scaled = scaler_0.fit_transform(X_0)\n",
        "X_1_scaled = scaler_1.fit_transform(X_1)\n",
        "\n",
        "print(f\"Class 0 scaled shape : {X_0_scaled.shape}\")\n",
        "print(f\"Class 1 scaled shape : {X_1_scaled.shape}\")"
      ],
      "metadata": {
        "id": "gwxyxzPlwPwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_distribution(labels, title, X_data=None):\n",
        "    counts = np.bincount(labels)\n",
        "    total = counts.sum()\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"  {title}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    for i, c in enumerate(counts):\n",
        "        print(f\"  Cluster {i:>2}: {c:>5} samples  ({c/total*100:.1f}%)\")\n",
        "    print(f\"  Total         : {total}\")\n",
        "    print(f\"  Max-Min Diff  : {counts.max() - counts.min()}\")\n",
        "    print(f\"  Balance Ratio : {counts.min()/counts.max():.3f}\")\n",
        "    if X_data is not None and len(np.unique(labels)) > 1:\n",
        "        score = silhouette_score(X_data, labels)\n",
        "        print(f\"  Silhouette    : {score:.4f}\")\n",
        "    return counts\n",
        "\n",
        "\n",
        "def run_constrained_kmeans(X_data, n_clusters, label, size_min=None, size_max=None):\n",
        "    n = len(X_data)\n",
        "    ideal = n // n_clusters\n",
        "    if size_min is None:\n",
        "        size_min = int(ideal * 0.75)\n",
        "    if size_max is None:\n",
        "        size_max = int(ideal * 1.25)\n",
        "\n",
        "    print(f\"\\n[{label}] n={n} | k={n_clusters} | size_min={size_min} | size_max={size_max}\")\n",
        "\n",
        "    kmeans = KMeansConstrained(\n",
        "        n_clusters=n_clusters,\n",
        "        size_min=size_min,\n",
        "        size_max=size_max,\n",
        "        random_state=42,\n",
        "        n_init=10\n",
        "    )\n",
        "    labels = kmeans.fit_predict(X_data)\n",
        "\n",
        "    counts = np.bincount(labels)\n",
        "    score = silhouette_score(X_data, labels)\n",
        "    balance_ratio = counts.min() / counts.max()\n",
        "\n",
        "    print(f\"  Silhouette Score : {score:.4f}\")\n",
        "    print(f\"  Balance Ratio    : {balance_ratio:.3f}\")\n",
        "    for i, c in enumerate(counts):\n",
        "        print(f\"  Cluster {i}: {c} samples ({c/n*100:.1f}%)\")\n",
        "\n",
        "    return labels, n_clusters, score"
      ],
      "metadata": {
        "id": "qtn08S57wha6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class 0\n",
        "labels_c0, k_c0, score_c0 = run_constrained_kmeans(\n",
        "    X_0_scaled,\n",
        "    n_clusters=2,\n",
        "    label=\"Class 0 - Initial\"\n",
        ")\n",
        "\n",
        "# Class 1\n",
        "labels_c1, k_c1, score_c1 = run_constrained_kmeans(\n",
        "    X_1_scaled,\n",
        "    n_clusters=2,\n",
        "    label=\"Class 1 - Initial\"\n",
        ")"
      ],
      "metadata": {
        "id": "QmIIjnkuwiv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_distribution(labels_c0, \"Class 0 — Initial Cluster Distribution\", X_0_scaled)\n",
        "show_distribution(labels_c1, \"Class 1 — Initial Cluster Distribution\", X_1_scaled)"
      ],
      "metadata": {
        "id": "Un2IptQBwpm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c0_cluster0_idx = np.where(labels_c0 == 0)[0]\n",
        "X_c0_cluster0 = X_0_scaled[c0_cluster0_idx]\n",
        "\n",
        "print(f\"Class 0 - Cluster 0 size: {len(c0_cluster0_idx)}\")\n",
        "\n",
        "re_labels_c0_0, re_k_c0_0, re_score_c0_0 = run_constrained_kmeans(\n",
        "    X_c0_cluster0,\n",
        "    n_clusters=4,\n",
        "    label=\"Class 0 - Cluster 0 Recluster\"\n",
        ")\n",
        "\n",
        "show_distribution(re_labels_c0_0, \"Class 0 — Cluster 0 Sub-clusters\", X_c0_cluster0)"
      ],
      "metadata": {
        "id": "dui4NTJMw03E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c0_cluster1_idx = np.where(labels_c0 == 1)[0]\n",
        "X_c0_cluster1 = X_0_scaled[c0_cluster1_idx]\n",
        "\n",
        "print(f\"Class 0 - Cluster 1 size: {len(c0_cluster1_idx)}\")\n",
        "\n",
        "re_labels_c0_1, re_k_c0_1, re_score_c0_1 = run_constrained_kmeans(\n",
        "    X_c0_cluster1,\n",
        "    n_clusters=4,\n",
        "    label=\"Class 0 - Cluster 1 Recluster\"\n",
        ")\n",
        "\n",
        "show_distribution(re_labels_c0_1, \"Class 0 — Cluster 1 Sub-clusters\", X_c0_cluster1)"
      ],
      "metadata": {
        "id": "evPOYas5w2Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_labels_class0 = np.full(len(X_0_scaled), -1, dtype=int)\n",
        "cluster_id = 0\n",
        "\n",
        "# Sub-clusters from Cluster 0\n",
        "for local_i, global_i in enumerate(c0_cluster0_idx):\n",
        "    final_labels_class0[global_i] = cluster_id + re_labels_c0_0[local_i]\n",
        "cluster_id += re_k_c0_0\n",
        "\n",
        "# Sub-clusters from Cluster 1\n",
        "for local_i, global_i in enumerate(c0_cluster1_idx):\n",
        "    final_labels_class0[global_i] = cluster_id + re_labels_c0_1[local_i]\n",
        "cluster_id += re_k_c0_1\n",
        "\n",
        "print(f\"Total final clusters for Class 0: {cluster_id}\")\n",
        "show_distribution(final_labels_class0, \"Class 0 — All Final Clusters Combined\", X_0_scaled)"
      ],
      "metadata": {
        "id": "106FUeZzxLqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store all final micro-cluster labels for Class 0\n",
        "final_labels_class0 = np.full(len(X_0_scaled), -1, dtype=int)\n",
        "cluster_id = 0\n",
        "\n",
        "# ---- From Cluster 0 re-labels (re_labels_c0_0) ----\n",
        "for sub_cluster in range(re_k_c0_0):\n",
        "    sub_idx_local = np.where(re_labels_c0_0 == sub_cluster)[0]\n",
        "    sub_idx_global = c0_cluster0_idx[sub_idx_local]\n",
        "    X_sub = X_0_scaled[sub_idx_global]\n",
        "\n",
        "    print(f\"\\nReclustering Class0 → Cluster0 → Sub{sub_cluster} | size={len(sub_idx_global)}\")\n",
        "\n",
        "    micro_labels, _, _ = run_constrained_kmeans(\n",
        "        X_sub,\n",
        "        n_clusters=2,\n",
        "        label=f\"C0-Cluster0-Sub{sub_cluster}\"\n",
        "    )\n",
        "\n",
        "    for local_i, global_i in enumerate(sub_idx_global):\n",
        "        final_labels_class0[global_i] = cluster_id + micro_labels[local_i]\n",
        "\n",
        "    cluster_id += 2\n",
        "\n",
        "# ---- From Cluster 1 re-labels (re_labels_c0_1) ----\n",
        "for sub_cluster in range(re_k_c0_1):\n",
        "    sub_idx_local = np.where(re_labels_c0_1 == sub_cluster)[0]\n",
        "    sub_idx_global = c0_cluster1_idx[sub_idx_local]\n",
        "    X_sub = X_0_scaled[sub_idx_global]\n",
        "\n",
        "    print(f\"\\nReclustering Class0 → Cluster1 → Sub{sub_cluster} | size={len(sub_idx_global)}\")\n",
        "\n",
        "    micro_labels, _, _ = run_constrained_kmeans(\n",
        "        X_sub,\n",
        "        n_clusters=2,\n",
        "        label=f\"C0-Cluster1-Sub{sub_cluster}\"\n",
        "    )\n",
        "\n",
        "    for local_i, global_i in enumerate(sub_idx_global):\n",
        "        final_labels_class0[global_i] = cluster_id + micro_labels[local_i]\n",
        "\n",
        "    cluster_id += 2\n",
        "\n",
        "print(f\"\\nTotal final micro-clusters for Class 0: {cluster_id}\")"
      ],
      "metadata": {
        "id": "pcDW_qDLxMzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_distribution(final_labels_class0, \"Class 0 — Final 16 Micro-Clusters\", X_0_scaled)"
      ],
      "metadata": {
        "id": "A1-mZ0S-xSxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n FINAL COMPARISON\")\n",
        "print(f\"\\nClass 0 — {cluster_id} clusters:\")\n",
        "counts_0 = np.bincount(final_labels_class0)\n",
        "for i, c in enumerate(counts_0):\n",
        "    print(f\"  Cluster {i:>2}: {c:>4} samples ({c/len(final_labels_class0)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nClass 1 — {k_c1} clusters (unchanged):\")\n",
        "counts_1 = np.bincount(labels_c1)\n",
        "for i, c in enumerate(counts_1):\n",
        "    print(f\"  Cluster {i:>2}: {c:>4} samples ({c/len(labels_c1)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nClass 0 avg per cluster : {counts_0.mean():.0f} samples\")\n",
        "print(f\"Class 1 avg per cluster : {counts_1.mean():.0f} samples\")\n",
        "print(f\"Class 0 balance ratio   : {counts_0.min()/counts_0.max():.3f}\")\n",
        "print(f\"Class 1 balance ratio   : {counts_1.min()/counts_1.max():.3f}\")"
      ],
      "metadata": {
        "id": "DxC6D3khxaFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx0 = y[y == 0].index\n",
        "idx1 = y[y == 1].index\n",
        "\n",
        "# Offset Class 1 labels to avoid overlap\n",
        "offset = final_labels_class0.max() + 1\n",
        "labels_c1_offset = labels_c1 + offset\n",
        "\n",
        "final_df = X.copy()\n",
        "cluster_series = pd.Series(index=final_df.index, dtype=int)\n",
        "cluster_series.loc[idx0] = final_labels_class0\n",
        "cluster_series.loc[idx1] = labels_c1_offset\n",
        "\n",
        "final_df['Cluster_Label'] = cluster_series\n",
        "final_df['Personal_Loan'] = y\n",
        "\n",
        "output_path = 'MicroClustered_bank_data.csv'\n",
        "final_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\" Saved : {output_path}\")\n",
        "print(f\"Shape   : {final_df.shape}\")\n",
        "print(f\"Unique Cluster Labels : {sorted(final_df['Cluster_Label'].unique())}\")\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "nycWGdh5xZyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('MicroClustered_bank_data.csv')"
      ],
      "metadata": {
        "id": "Kjek9pnpxma1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}